{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>MACHINE LEARNING <br> <br> Problem Session 7 <br> Linear and Logistic models</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import seaborn as sns\n",
    "from mpl_toolkits import mplot3d\n",
    "from mpl_toolkits.mplot3d.axes3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def constructDesignMatrix1D(x,deg):\n",
    "    X = np.ones(x.shape)\n",
    "    \n",
    "    for i in range(deg):\n",
    "        X = np.hstack((X,x**(i+1)))\n",
    "    \n",
    "    return X\n",
    "\n",
    "def computeError(X,y,w):\n",
    "    # computeCost : average sum of squares error. Expects\n",
    "    #     X : NxD design matrix\n",
    "    #     t : Nx1 target response variable\n",
    "    #     theta : model parameters\n",
    "    N = len(y)\n",
    "    aux = np.dot(X,w) - y\n",
    "    return np.sum( aux**2 , axis = 0 ) / 2\n",
    "\n",
    "\n",
    "def gradientDescent_RidgeReg(X, y, w, alpha, reg, maxIts,tol):\n",
    "    # gradientDescent expects \n",
    "    #     X : matrix of shape NxD\n",
    "    #     y : vector of shape Nx1\n",
    "    #     alpha : learning rate\n",
    "    #     reg :  lambda, regularisation parameter\n",
    "    #     maxIts : max number of iterations\n",
    "    #     tol : absolute tolerance error\n",
    "\n",
    "    w_vec = np.zeros([len(w),maxIts])\n",
    "    error_vec = np.zeros([maxIts])\n",
    "    N = len(y) # number of training examples\n",
    "    \n",
    "    for it in range(maxIts):\n",
    "        # error for current parameters theta : w^T * X - y\n",
    "        error = np.dot(X, w) - y\n",
    "        \n",
    "        # gradient : X^T * error\n",
    "        grad = np.dot(X.T, error)\n",
    "        \n",
    "        # update : w = w - alpha/N * grad - alpha * reg * w\n",
    "        w = w - (alpha/N) * grad - alpha * reg * w\n",
    "        \n",
    "        # save current it params and error\n",
    "        w_vec[:,it] = np.reshape(w,(len(w),))\n",
    "        error_vec[it] = computeError(X, y, w)\n",
    "        \n",
    "        # check convergence\n",
    "        if it > 0 and abs( error_vec[it] - error_vec[it-1] ) < tol:\n",
    "            return w_vec[:,:it+1], error_vec[:it+1]\n",
    "        \n",
    "\n",
    "    return w_vec, error_vec\n",
    "\n",
    "\n",
    "\n",
    "def plotModel1D(w,x):\n",
    "    xL = np.min(x) - abs(np.max(x)-np.min(x))/2\n",
    "    xR = np.max(x) + abs(np.max(x)-np.min(x))/2\n",
    "    x_plot = np.reshape(np.linspace(xL,xR,50), (50,x.shape[1]))\n",
    "    \n",
    "    deg = len(w) - 1\n",
    "    X_plot = np.ones(x_plot.shape)\n",
    "    for i in range(deg):\n",
    "        X_plot = np.hstack((X_plot,x_plot**(i+1)))\n",
    "            \n",
    "    plt.plot(x_plot,np.dot(X_plot,w),'-')\n",
    "    \n",
    "\n",
    "def plotError3D(X,y,w,ax,**kwargs):\n",
    "    \n",
    "    error = computeError(X,y,w)\n",
    "    aux = error < 1e5\n",
    "    error = error[aux]\n",
    "    w = w[:,aux]\n",
    "    \n",
    "    # Plot w evolution\n",
    "    if w.shape[1] < 3:\n",
    "        ax.scatter3D(w[0,:],w[1,:],errors,s=20,c='r')\n",
    "        if w.shape[1] > 1:\n",
    "            vec = np.hstack((w[:,1],errors[1])) - np.hstack((w[:,0],errors[0]))\n",
    "            ax.quiver3D(w[0,0], w[1,0], errors[0], vec[0] ,vec[1], vec[2], arrow_length_ratio=.0001)\n",
    "    else:\n",
    "        ax.scatter3D(w[0,:],w[1,:],error,s=5,c = range(len(error)))\n",
    "    \n",
    "    \n",
    "    # Plot E(w) surface\n",
    "    w0L = np.min(w[0,:]) - abs(np.max(w[0,:])-np.min(w[0,:]))/2\n",
    "    w0R = np.max(w[0,:]) + abs(np.max(w[0,:])-np.min(w[0,:]))/2\n",
    "    w1L = np.min(w[1,:]) - abs(np.max(w[1,:])-np.min(w[1,:]))/2\n",
    "    w1R = np.max(w[1,:]) + abs(np.max(w[1,:])-np.min(w[1,:]))/2\n",
    "    \n",
    "    \n",
    "    for keyWord, kw_value in kwargs.items():\n",
    "        if keyWord == 'w0Lims':\n",
    "            w0L = kw_value[0]\n",
    "            w0R = kw_value[1]\n",
    "        elif keyWord == 'w1Lims':\n",
    "            w1L = kw_value[0]\n",
    "            w1R = kw_value[1]\n",
    "        else:\n",
    "            raise NameError('Undefined input argument.')\n",
    "    \n",
    "    \n",
    "    w0 = np.linspace(w0L, w0R, 50)\n",
    "    w1 = np.linspace(w1L, w1R, 50)\n",
    "    w0, w1 = np.meshgrid(w0, w1)\n",
    "\n",
    "    errorMesh = computeError( X, y, np.array([w0.flatten(),w1.flatten()]) )\n",
    "    errorMesh = np.reshape(errorMesh,w0.shape)\n",
    "\n",
    "    ax.plot_surface(w0, w1, errorMesh, cmap=cm.coolwarm, linewidth=0, antialiased=False, alpha = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "Consider the following training set $\\left\\{ \\left(x^{(i)}, y^{(i)}\\right) \\right\\}_{i = 1}^5$ with feature variables $\\left\\{ x^{(1)} = 10, x^{(2)} = 10, x^{(3)} = 20, x^{(4)} = 35, x^{(5)} = 40\\right\\}$ and target variables $\\left\\{ y^{(1)} = 10, y^{(2)} = 20, y^{(3)} = 20, y^{(4)} = 25, y^{(5)} = 35 \\right\\}$.\n",
    "\n",
    "a) *Closed-form solution*\n",
    "\n",
    "i. Estimate the parameters of a Linear Regression model using the closed-form solution and the parameters when a Ridge regression is applied, using also the closed-form solution, with $\\lambda = 1$.\n",
    "\n",
    "\n",
    "ii. Compute the error in the training set for both models and plot the training set and both models in the same figure.\n",
    "\n",
    "What differences are between the models? What is the effect of the regularisation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x = np.array([[10,10,20,35,40]]).T # column vector, each entry is a sample\n",
    "y = np.array([[10,20,20,25,30]]).T # column vector, each entry is the ground truth response of a sample\n",
    "deg = 1 # degree of model to be fitted\n",
    "\n",
    "# Construct design matrix [1,x,x^2,x^3,...,x^deg]\n",
    "X_mat = constructDesignMatrix1D(x,deg)\n",
    "\n",
    "# Closed-form solution w/o regularisation\n",
    "w_cf = np.dot(np.linalg.inv(np.dot(X_mat.T, X_mat)), np.dot(X_mat.T, y))\n",
    "print('w without regularisation = {}'.format(w_cf.T))\n",
    "\n",
    "# Closed-form solution with regularisation, lambda = 1\n",
    "lReg = 1\n",
    "wReg_cf = np.dot(np.linalg.inv(np.dot(X_mat.T, X_mat) + (lReg)*np.eye(deg+1)), np.dot(X_mat.T, y))\n",
    "print('w with regularisation = {}'.format(wReg_cf.T))\n",
    "\n",
    "# Errors\n",
    "print('Error without regularisation = {}'.format(computeError(X_mat,y,w_cf)))\n",
    "print('Error with regularisation = {}'.format(computeError(X_mat,y,wReg_cf)))\n",
    "\n",
    "# Plot both models\n",
    "fig = plt.figure(figsize=(5,5))\n",
    "plt.scatter(x,y)\n",
    "plotModel1D(w_cf,x)\n",
    "plotModel1D(wReg_cf,x)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.axis('equal')\n",
    "plt.grid('on')\n",
    "plt.legend(['Without reg', 'With reg'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Compute a Gradient Descent update for a **Linear regression model**. Consider initial parameters $w_0 = 0, w_1 = 0$ and learning rate $\\alpha = 0.001$.\n",
    "\n",
    "i. Write the parametric form of a linear regression model $g(\\mathbf{x},\\mathbf{w})$ and the error function that is minimised when fitting a linear model to the **given data**.\n",
    "   \n",
    "ii. Find the expression of the gradient descent update of the parameters $\\mathbf{w}$. To do so, derive the error function from (i) with respect to each of the parameters $w_j$.\n",
    "\n",
    "iii. Consider the initial parameters $\\mathbf{w} = \\left(0,0\\right)^\\text{T}$ and a learning rate $\\alpha = 10^{-4}$. Update the parameters with the gradient descent rule. Plot the initial and the updated models in the same figure as the training set.\n",
    "\n",
    "iv. Compute the error for the initial parameters and for the updated ones. Draw in 3D (axes $w_0,w_1,E(w_0,w_1)$) the initial parameters and error, and the updated parameters with the current error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# (iii)\n",
    "print('-----------------------------------------------------------------------------')\n",
    "print('(iii)')\n",
    "\n",
    "wOld = np.array([[0,0]]).T\n",
    "alpha = 1e-4\n",
    "\n",
    "# Parameters update\n",
    "wNew = wOld - alpha * ( np.dot(X_mat.T, np.dot(X_mat,wOld) - y ) )\n",
    "\n",
    "# Plot old and updated models\n",
    "fig = plt.figure(figsize=(5,5))\n",
    "plt.scatter(x,y)\n",
    "plotModel1D(wOld,x)\n",
    "plotModel1D(wNew,x)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.axis('equal')\n",
    "plt.grid('on')\n",
    "plt.legend(['it = 0', 'it = 1'])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# (iv)\n",
    "print('-----------------------------------------------------------------------------')\n",
    "print('(iv)')\n",
    "w = np.hstack((wOld,wNew))\n",
    "\n",
    "# Compute errors\n",
    "errors = computeError(X_mat,y,w)\n",
    "print('Error at it = 0: {}'.format(errors[0]))\n",
    "print('Error at it = 1: {}'.format(errors[1]))\n",
    "\n",
    "\n",
    "# Plot the step\n",
    "fig = plt.figure(figsize=(5,5))\n",
    "ax = plt.gca(projection='3d')\n",
    "ax.set_xlabel('w_0')\n",
    "ax.set_ylabel('w_1')\n",
    "ax.set_zlabel('Error(w)')\n",
    "plotError3D(X_mat,y,w,ax)\n",
    "plt.show()\n",
    "\n",
    "ax.view_init(20, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Estimate the parameters of a **Linear Regression model** using the Gradient Descent, and the parameters when a Ridge regression is applied, using also Gradient Descent, with $\\lambda = 500$ (this value is too large, but consider it an experiment to see the effect of the regularisation). Consider a tolerance of $10^{-6}$, a learning rate of $10^{-4}$, and a maximum number of iterations of $5000$.\n",
    "\n",
    "(i) Compute the error in the training set for both models and plot the training set and both models in the same figure. Also, create another figure with the evolution of the parameters with respect to the error (3D plot). Use these two figures and the value of the errors to observe the effect of the regularisation. Which conclusion do you extract? Reason your answer.\n",
    "\n",
    "(ii) Try different values of the learning rate, $\\alpha = 10^{-6}, 10^{-4}, 3\\cdot10^{-3}$ and plot the evolution of the parameters with respect to the error for each of the cases. What is the difference between all the learning parameters? Reason your answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "w0 = np.array([[0,0]]).T\n",
    "alpha = 1e-6\n",
    "nIts = 5000\n",
    "tol = 1e-4\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------------------------------------------\n",
    "# (i)\n",
    "print('-----------------------------------------------------------------------------')\n",
    "print('(i)')\n",
    "\n",
    "lReg = 0\n",
    "w_vec, error_vec = gradientDescent_RidgeReg(X_mat, y, w0, alpha, lReg, nIts, tol)\n",
    "lReg = 500\n",
    "wReg_vec, errorReg_vec = gradientDescent_RidgeReg(X_mat, y, w0, alpha, lReg, nIts, tol)\n",
    "\n",
    "print('w without regularisation: {}'.format(w_vec[:,-1]))\n",
    "print('w with regularisation: {}'.format(wReg_vec[:,-1]))\n",
    "\n",
    "\n",
    "# Compute errors\n",
    "print('Error without regularisation = {}'.format( error_vec[-1] ) )\n",
    "print('Error with regularisation = {}'.format( errorReg_vec[-1] ) )\n",
    "\n",
    "# Models plot\n",
    "fig = plt.figure(figsize=(5,5))\n",
    "plt.scatter(x,y)\n",
    "plotModel1D(w0,x)\n",
    "plotModel1D(w_vec[:,-1],x)\n",
    "plotModel1D(wReg_vec[:,-1],x)\n",
    "plt.axis('equal')\n",
    "plt.grid('on')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend(['Initial','Without reg', 'With reg'])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Errors plot\n",
    "fig = plt.figure(figsize=(5,5))\n",
    "ax = plt.gca(projection='3d')\n",
    "ax.set_xlabel('w_0')\n",
    "ax.set_ylabel('w_1')\n",
    "ax.set_zlabel('Error(w)')\n",
    "plotError3D(X_mat,y,w_vec,ax)\n",
    "ax.scatter3D(wReg_vec[0,:],wReg_vec[1,:],errorReg_vec,s=30,c = range(len(errorReg_vec)))\n",
    "plt.show()\n",
    "\n",
    "ax.view_init(20, 20)\n",
    "plt.draw()\n",
    "\n",
    "\n",
    "# Errors plot with closed form solution\n",
    "fig = plt.figure(figsize=(5,5))\n",
    "ax = plt.gca(projection='3d')\n",
    "ax.set_xlabel('w_0')\n",
    "ax.set_ylabel('w_1')\n",
    "ax.set_zlabel('Error(w)')\n",
    "plotError3D(X_mat,y,w_vec,ax, w0Lims = [-2,11], w1Lims = [-0.5,1.5])\n",
    "ax.scatter3D(wReg_vec[0,:],wReg_vec[1,:],errorReg_vec,s=30,c = range(len(errorReg_vec)))\n",
    "ax.scatter3D(w_cf[0,:],w_cf[1,:],computeError(X_mat,y,w_cf),s=30,c = 'r')\n",
    "plt.show()\n",
    "\n",
    "ax.view_init(20, 20)\n",
    "plt.draw()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------------------------------------------\n",
    "#(ii)\n",
    "print('-----------------------------------------------------------------------------')\n",
    "print('(ii)')\n",
    "\n",
    "lReg = 0\n",
    "\n",
    "alpha = 1e-6\n",
    "w_vec, error_vec = gradientDescent_RidgeReg(X_mat, y, w0, alpha, lReg, nIts, tol)\n",
    "\n",
    "fig = plt.figure(figsize=(15,5) ) \n",
    "ax = fig.add_subplot(1, 3, 1, projection='3d')\n",
    "ax.set_xlabel('w_0')\n",
    "ax.set_ylabel('w_1')\n",
    "ax.set_zlabel('Error(w)')\n",
    "ax.set_title('alpha = 1e-6')\n",
    "plotError3D(X_mat,y,w_vec,ax)\n",
    "\n",
    "ax.view_init(30, 0)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "alpha = 1e-4\n",
    "w_vec, error_vec = gradientDescent_RidgeReg(X_mat, y, w0, alpha, lReg, nIts, tol)\n",
    "\n",
    "ax = fig.add_subplot(1, 3, 2, projection='3d')\n",
    "ax.set_xlabel('w_0')\n",
    "ax.set_ylabel('w_1')\n",
    "ax.set_zlabel('Error(w)')\n",
    "ax.set_title('alpha = 1e-4')\n",
    "plotError3D(X_mat,y,w_vec,ax)\n",
    "\n",
    "ax.view_init(30, 0)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "alpha = 3e-3\n",
    "w_vec, error_vec = gradientDescent_RidgeReg(X_mat, y, w0, alpha, lReg, nIts, tol)\n",
    "\n",
    "ax = fig.add_subplot(1, 3, 3, projection='3d')\n",
    "ax.set_xlabel('w_0')\n",
    "ax.set_ylabel('w_1')\n",
    "ax.set_zlabel('Error(w)')\n",
    "ax.set_title('alpha = 3e-3')\n",
    "plotError3D(X_mat,y,w_vec,ax)\n",
    "\n",
    "ax.view_init(30, 0)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "Consider the dataset given in the attached CSV file. Fit the best possible regression model to the given dataset using gradient descent. To do so, you will have to:\n",
    "\n",
    " - Choose the degree of the polynomial you want to fit.\n",
    " - Decide whether to use regularisation or not. In the positive case, choose the value of the regularisation coefficient, $\\lambda$.\n",
    " - Choose an optimal learning rate, $\\alpha$.\n",
    " \n",
    "Justify your choices and add figures to support them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('dataEx5.csv')\n",
    "\n",
    "x = dataset['x'].to_numpy()\n",
    "x = np.reshape(x,(len(x),1))\n",
    "\n",
    "y = dataset['y'].to_numpy()\n",
    "y = np.reshape(y,(len(y),1))\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(5,5))\n",
    "plt.scatter(x,y)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.axis('equal')\n",
    "plt.grid('on')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "482px",
    "left": "1528px",
    "right": "20px",
    "top": "21px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
