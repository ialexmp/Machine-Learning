{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>MACHINE LEARNING <br> <br> Problem Session 8 <br> Logistic Regression and SoftMax</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "e) Generate a gradient descend algorithm with ridge regularisation. To do so, you only have to add to the gradient an extra term $+\\lambda w$, where $\\lambda$ is the regularisation parameter. Plot the data points and the resulting linear classifier in the same figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def sigmoid(X,w):\n",
    "    # sigmoid: Computes the sigmoid function\n",
    "    #     X : NxD design matrix\n",
    "    return 1/(1+np.exp(-np.dot(w,X.T)))\n",
    "\n",
    "def computeError(X,y,w):\n",
    "    # computeCost : average sum of squares error. Expects\n",
    "    #     X : NxD design matrix\n",
    "    #     t : Nx1 target response variable\n",
    "    #     theta : model parameters\n",
    "    N = len(y)\n",
    "    h=sigmoid(X,w) #sigmoid function\n",
    "    aux =  y*np.log(h)+(1-y)*np.log(1-h)\n",
    "    return np.sum( aux**2 , axis = 0 ) / 2\n",
    "\n",
    "\n",
    "def gradientDescent_Logistic_Ridge(X, y, w, alpha, reg, maxIts,tol):\n",
    "    # gradientDescent expects \n",
    "    #     X : matrix of shape NxD\n",
    "    #     y : vector of shape Nx1\n",
    "    #     alpha : learning rate\n",
    "    #     reg :  lambda, regularisation parameter\n",
    "    #     maxIts : max number of iterations\n",
    "    #     tol : absolute tolerance error\n",
    "\n",
    "    w_vec = np.zeros([len(w),maxIts])\n",
    "    error_vec = np.zeros([maxIts])\n",
    "    N = len(y) # number of training examples\n",
    "    \n",
    "    for it in range(maxIts):\n",
    "        # error for current parameters theta : w^T * X - y\n",
    "        error = computeError(X,y,w)\n",
    "        \n",
    "        # gradient : X^T * error\n",
    "        grad = np.dot((sigmoid(X,w)-Y),X)\n",
    "        \n",
    "        # update : w = w - alpha/N * grad - alpha * reg * w\n",
    "        w = w - alpha * grad - alpha * reg * w\n",
    "        \n",
    "        # save current it params and error\n",
    "        w_vec[:,it] = np.reshape(w,(len(w),))\n",
    "        error_vec[it] = computeError(X, y, w)\n",
    "        \n",
    "        # check convergence\n",
    "        if it > 0 and abs( error_vec[it] - error_vec[it-1] ) < tol:\n",
    "            return w_vec[:,:it+1], error_vec[:it+1]\n",
    "        \n",
    "\n",
    "    return w_vec, error_vec\n",
    "\n",
    "\n",
    "def plotModel(w,X,Y):\n",
    "    x_min=np.min(X[:,1])\n",
    "    x_max=np.max(X[:,1])\n",
    "    \n",
    "    x = np.linspace(x_min,x_max,100)\n",
    "\n",
    "    m=-w[1]/w[2]\n",
    "    n=-w[0]/w[2]\n",
    "    y = m*x+n\n",
    "    plt.plot(x, y)\n",
    "    plt.scatter(X[:,1],X[:,2],c=Y)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " f) (optional) Estimate the logistic classifier using the function https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html}{sklearn.linear\\_model.LogisticRegression. Draw the data points and the resulting linear classifier in the same figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4\n",
    "\n",
    "a) We will fit the model $p(y=1 | \\mathbf{x}, \\mathbf{w})=\\sigma(w_0 +w_1 x_1 +w_2 x_2)$ by minimizing the Cross-Entropy error $ \\mathbb{E}(\\mathbf{w})$.  For the solution you have obtained, How many points are wrong classified?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=np.array([[1.,2.,6.], [1.,3.,6.], [1.,4.,6.],[1,4,7],[1,3,5],[1,3,1],[1,6,2],[1,6,3],[1,6,5],[1,7,2],[1,7,3],[1,8,2],[1,8,3]])\n",
    "Y=np.array([1,1,1,1,1,1,0,0,0,0,0,0,0])\n",
    "w=np.array([0.,0.,0.])\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Now suppose we regularize only the $w_{0}$ parameter, i.e, we minimize $\\mathbb{E}_T(\\mathbf{w})=\\mathbb{E}(\\mathbf{w})+ \\lambda w_{0}^{2}$.\n",
    "            \n",
    "  Use the Gradient Descent to estimate the classifier. Comment the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
